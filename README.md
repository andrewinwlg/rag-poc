# Local RAG POC

A local Retrieval-Augmented Generation (RAG) system using Docker for running entirely on your PC. This system provides:

- **PostgreSQL** with `pgvector` extension for efficient vector similarity search
- **Python encoder** that chunks and embeds markdown/source code files using Sentence Transformers
- **Ollama** serving a lightweight local LLM (e.g., `gemma:2b`)

---

## ğŸš€ Quick Start

### 1. Prerequisites
Ensure you have the following installed:
- **Docker** and **Docker Compose** (v2.x recommended)
- **Python 3.10+** with pip
- At least **6-8 GB RAM** and **2-4 CPU cores**

### 2. Clone and Setup
```bash
git clone <your-repo-url>
cd rag-poc
```

### 3. Add Your Knowledge Base
Place your documents in the `data/` folder:
```bash
# Example: Add some sample files
echo "# Welcome to RAG\nThis is a sample document for testing." > data/sample.md
echo "# Another Doc\nThis contains more information about the system." > data/guide.md
```

### 4. Install Python Dependencies
```bash
pip install sentence-transformers psycopg2-binary requests
```

### 5. Start the System
```bash
docker compose up --build
```

**What happens:**
1. PostgreSQL starts with pgvector extension
2. Encoder service processes your documents and creates embeddings
3. Ollama downloads and serves the gemma:2b model

### 6. Query Your Knowledge Base
Once all services are running (wait for "encoder_1 exited with code 0"):
```bash
python rag.py "What is this system about?"
```

---

## ğŸ§  How It Works

```
Your Question â†’ Sentence Transformer â†’ Vector Search (PostgreSQL) â†’
   â†’ Retrieve Similar Chunks â†’ LLM Prompt â†’ Generated Answer
```

1. **Document Processing**: Files in `data/` are chunked and embedded using MiniLM
2. **Vector Storage**: Embeddings stored in PostgreSQL with pgvector for fast similarity search
3. **Query Processing**: Your question is embedded and matched against stored chunks
4. **Answer Generation**: Retrieved context is sent to Ollama's LLM for final answer

---

## ğŸ“ Project Structure

```
rag-poc/
â”œâ”€â”€ data/                   # ğŸ“ Your knowledge base files (.md, .py)
â”œâ”€â”€ encoder/
â”‚   â”œâ”€â”€ Dockerfile          # ğŸ³ Python environment for embedding
â”‚   â””â”€â”€ embed.py           # ğŸ”§ Document processing script
â”œâ”€â”€ pgvector/
â”‚   â””â”€â”€ Dockerfile         # ğŸ³ PostgreSQL with vector extension
â”œâ”€â”€ docker-compose.yml     # ğŸ³ Service orchestration
â”œâ”€â”€ rag.py                 # ğŸ” Main query interface
â”œâ”€â”€ requirements.txt       # ğŸ“¦ Python dependencies
â””â”€â”€ README.md             # ğŸ“– This file
```

---

## ğŸ”§ Configuration & Customization

### Change the LLM Model
Edit `rag.py` line 31 to use different models:
```python
"model": "mistral",     # or "llama2", "codellama", etc.
```

### Adjust Chunk Settings
Modify `encoder/embed.py` line 28:
```python
def chunk_text(text, chunk_size=500, overlap=50):  # Experiment with these values
```

### Add More File Types
Extend `encoder/embed.py` to support PDFs, DOCX, etc.:
```python
# Add to the file type check:
if filepath.suffix not in [".md", ".py", ".txt", ".pdf"]:
```

---

## ğŸš¨ Troubleshooting

### Service Won't Start
```bash
# Check Docker status
docker compose ps

# View logs for specific service
docker compose logs pgvector
docker compose logs encoder
docker compose logs ollama
```

### "Connection Refused" Error
- Ensure all services are running: `docker compose ps`
- Wait for Ollama to download the model (first run takes time)
- Check if ports 5432 and 11434 are available

### Out of Memory
- Reduce chunk size in `embed.py`
- Use a smaller model like `gemma:2b` instead of larger ones
- Ensure Docker has enough memory allocated (6GB+)

### No Documents Found
- Verify files are in `data/` folder
- Check encoder logs: `docker compose logs encoder`
- Ensure file extensions are `.md` or `.py`

---

## ğŸ“Š Performance Notes

| Component | Resource Usage | Notes |
|-----------|---------------|-------|
| PostgreSQL | ~200MB RAM | Scales with document count |
| Encoder | ~1GB RAM | Temporary, exits after processing |
| Ollama | ~4GB RAM | Persistent, loads model in memory |
| **Total** | **~6GB RAM** | Plus your documents |

**First Run**: May take 5-10 minutes to download the LLM model
**Subsequent Runs**: ~30 seconds to start all services

---

## ğŸ›  Development & Production

### For Development
- Add more sophisticated chunking strategies
- Implement reranking for better retrieval
- Add support for more file formats
- Build a web UI with FastAPI + React

### For Production
- Add authentication and rate limiting
- Implement proper error handling and logging
- Use managed PostgreSQL service
- Add monitoring and health checks

---

## ğŸ’¡ Tips & Best Practices

- **Documents**: Keep files focused and well-structured for better chunking
- **Queries**: Be specific in your questions for better results
- **Models**: Start with `gemma:2b`, upgrade to larger models if needed
- **Chunks**: Smaller chunks = more precise, larger chunks = more context

---

## ğŸ“œ License

MIT License - Feel free to use and modify for your projects!

---

*Built with â¤ï¸ for local AI experimentation*
